# Messages

## Introduction

Messages are at the core of LangChainJS.

- When invoking a model / agent you pass in one of more messages to the model / agent
- As a result you get back one or more messages from the model / agent
- Messages are the way you communicate with the model / agent
- Messages can contain metadata and can be used to group messages together in a thread
- There are different types of messages

Each message type has its own purpose and structure but they all contain

- A role (System, Human, AI, Tool)
- Content (Text, Images, Audio, Video, Files)
- MetaData (Timestamp, ID, ThreadID, ...)

## Types of Messages

There are different types of messages:

- SystemMessage
- HumanMessage
- AIMessage
- ToolMessage

### SystemMessage

Initial set of instructions to the model. Defines its context and how it should behave.

### HumanMessage

Represents the user input to the model.

#### Human Message Structure

```
    HumanMessage {
      "id": "ded28a89-9d73-46de-b75b-c8ec6562435c",
      "content": [
        {
          "type": "text",
          "text": "\"what temperature is it in Florida ?\""
        }
      ],
      ...
    },
```

### AIMessage

An AIMessage represents a message generated by an AI model.
Consider it an output that you get back from the model typically in response to a HumanMessage.


#### AI Message Structure

Here's the structure of an AIMessage:
```
AIMessage {
  "id": "chatcmpl-CgIFvH7V5tfxNvLFOQ8VAIyMja5OA",
  "content": "Hello! How can I assist you today?",
  "additional_kwargs": {},
  "response_metadata": {
    "tokenUsage": {
      "promptTokens": 9,
      "completionTokens": 9,
      "totalTokens": 18
    },
    "finish_reason": "stop",
    "model_provider": "openai",
    "model_name": "gpt-4o-mini-2024-07-18",
    "usage": {
      "prompt_tokens": 9,
      "completion_tokens": 9,
      "total_tokens": 18,
      "prompt_tokens_details": {
        "cached_tokens": 0,
        "audio_tokens": 0
      },
      "completion_tokens_details": {
        "reasoning_tokens": 0,
        "audio_tokens": 0,
        "accepted_prediction_tokens": 0,
        "rejected_prediction_tokens": 0
      }
    },
    "system_fingerprint": "fp_560af6e559"
  },
  "tool_calls": [],
  "invalid_tool_calls": [],
  "usage_metadata": {
    "output_tokens": 9,
    "input_tokens": 9,
    "total_tokens": 18,
    "input_token_details": {
      "audio": 0,
      "cache_read": 0
    },
    "output_token_details": {
      "audio": 0,
      "reasoning": 0
    }
  }
}

```

### ToolMessage

A ToolMessage represents a message generated by a tool.
When the LLM has tools bound to it it can call those tools and the output of the tool will be represented as a ToolMessage.
Imagine we have a tool to get the weather for a certain city.

If the LLM decides it needs to call the tool it will first generate an AIMessage with the tool call.

As you can see there it has decided to call the tool `get_weather` with the argument `city` with a value `Florida`:

#### AI Message with Tool Call Structure
```
AIMessage {
  "id": "run-58633f65-6565-44d6-b3c6-d1205204e4d1",
  ...
  "tool_calls": [
    {
      "name": "get_weather",
      "args": {
        "city": "Florida"
      },
      "type": "tool_call",
      "id": "call_XIKPzO89ZRVMDr3C6aj3HMwO"
    }
  ],
  "invalid_tool_calls": [],
  "usage_metadata": {
    ...
  }
},
```

The LLM itself will not call the tool directly. It depends on some kind of orchestrator (in this case LangChain) to go ahead and call the tool.
LangChain will execute the tool (the piece of typescript code from our get_weather function) and return a `ToolMessage`

#### ToolMessage Structure

This `ToolMessage` will contain the output of the tool 
```
    ToolMessage {
      "id": "2d843dba-8aca-4fbe-a0f3-3832a6d31349",
      "content": "30",
      "name": "get_weather",
      "additional_kwargs": {},
      "response_metadata": {},
      "tool_call_id": "call_XIKPzO89ZRVMDr3C6aj3HMwO"
    },
```

This is what allows the LLM to respond to the user with an AIMessage. 
It has taken the content from the ToolMessage and has incorporated it into the AIMessage. (the final response to the user)

```
    AIMessage {
      "id": "msg_0c76ae1e8f0ce8950069472b135e2c8196b6c804b388c3b6b5",
      "content": [
        {
          "type": "text",
          "text": "The temperature in Florida is currently 30Â°C.",
          "annotations": []
        }
      ],
      ...
    }
```

### Message Chunks

When you stream model responses you will get back multiple messages called chunks

These are called `AIMessageChunk` objects
These are just like AIMessages but they contain a small portion of the content

```
{
    "id": "chatcmpl-Cp0iUEzoGWKND5cVptdNCGeysDMQp",
    "content": " today",
    "additional_kwargs": {},
    "response_metadata": {
      "model_provider": "openai",
      "usage": {}
    },
    "tool_calls": [],
    "tool_call_chunks": [],
    "invalid_tool_calls": []
    },
    {
    tags: [],
    name: undefined,
    langgraph_step: 1,
    ...
}
```

The buildup of the actual tool call is done by the LLM and is also done in chucnks

```
"tool_call_chunks": [{"args": "{\"","index": 0,"type": "tool_call_chunk"}],
"tool_call_chunks": [{"args": "city","index": 0,"type": "tool_call_chunk"}],
"tool_call_chunks": [{"args": "\":\"","index": 0,"type": "tool_call_chunk"}],
"tool_call_chunks": [{"args": "Florida","index": 0,"type": "tool_call_chunk"}],
....
```

Chunks can also contain a `ToolMessage`

```
{
    "id": "run-c1fc8476-ada4-418f-9c52-2e9ec819dca7-tool-call_p6bgV9YiGgapSTJ3QvDgx6g6",
    "content": "30",
    "name": "get_weather",
    "additional_kwargs": {},
    "response_metadata": {},
    "tool_call_id": "call_p6bgV9YiGgapSTJ3QvDgx6g6"
}
```


And AIMessage chunks can contain a tool call chunk

```
  AIMessageChunk {
    "id": "chatcmpl-Cp0mjqHhS0Qp15J4QZbyq1JCdRFGW",
    "content": "",
    "additional_kwargs": {
      "tool_calls": [
        {
          "index": 0,
          "id": "call_p6bgV9YiGgapSTJ3QvDgx6g6",
          "type": "function",
          "function": "[Object]"
        }
      ]
    },
    "response_metadata": {
      "model_provider": "openai",
      "usage": {}
    },
    "tool_calls": [
      {
        "name": "get_weather",
        "args": {},
        "id": "call_p6bgV9YiGgapSTJ3QvDgx6g6",
        "type": "tool_call"
      }
    ],
    "tool_call_chunks": [
      {
        "name": "get_weather",
        "args": "",
        "id": "call_p6bgV9YiGgapSTJ3QvDgx6g6",
        "index": 0,
        "type": "tool_call_chunk"
      }
    ],
    "invalid_tool_calls": []
  },

```








## Creating Messages

There are different ways to create messages.

- String / Array of Strings
- OpenAI Completions Format
- LangChain Messages


### Strings / Array of Strings

```
model.invoke("Tell me a joke.");
```
or 
```
model.invoke(["Hello, how are you?", "Tell me a joke."]);
```


### OpenAI Completions Format

```
model.invoke([
        {role: "system", content: "You will act and respond like a millitary drill sergeant."},
        {role: "user", content: "Hello, how are you?"}
    ]);
```

### LangChain Messages

```
model.invoke([
        new SystemMessage("You are a millitary expert"),
        new AIMessage("You will act and respond like a millitary drill sergeant."),
        new HumanMessage("Hello, how are you?")
    ]);
```    

# Summary

So to summarize we can have something like this 

```
    const langchainMessages = [
        new SystemMessage("You are an expert in Typescript"),
        new AIMessage("You will be able to answer questions about Typescript."),
        new HumanMessage("What is the difference between a type and an interface?")
    ];

    const response = await model.invoke(langchainMessages);
```

# References

https://docs.langchain.com/oss/javascript/langchain/messages